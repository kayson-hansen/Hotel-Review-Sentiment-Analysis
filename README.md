# TripAdvisor Hotel Review Sentiment Analysis

Motivation:
We created a binary classification sentiment analysis machine learning model on TripAdvisor hotel reviews in Las Vegas. By doing this analysis, we hope to be able to better understand the type of language people use when describing their opinions on products and experiences and whether reviews for casinos are generally positive or negative. The inputs to our algorithm were sentence embedding vectors derived from taking the mean of all of the word embeddings in a review—word embeddings were generated using Spacy. Since we are only interested in positive and negative sentiments of reviews, we disregarded reviews that were given 3 stars and only used 1 and 2-star reviews (negative sentiment) and 4 and 5-star reviews (positive sentiment) for our inputs. We then used a neural network to output the probability that the review has a positive sentiment (1) or a negative sentiment (0). 

Dataset:
First, we scraped TripAdvisor in order to obtain our datasets. We hypothesized that Las Vegas hotels and casinos would have a high volume of reviews for each hotel, so we chose 15 Las Vegas Hotels to scrape for reviews. The number of reviews for these hotels ranged from approximately 10,000 to 30,000. In total, we had approximately 150,000 reviews. We used a 80/10/10 split of training / cross validation / test set data, so we had around 120,000 training examples, 15,000 cross validation examples, and 15,000 test set examples. Each set included reviews from various hotels, so we would not have, for example, all of the test set reviews being from the same hotel. We performed analysis on our data by finding the total number of reviews of each star, and we computed the mean and standard deviation of the ratings. There were 16,109 1-star reviews, 12,720 2-star reviews, 19,906 3-star reviews, 33,156 4-star reviews, and 82,908 5-star reviews. The average review was 3.93 stars. The standard deviation was 1.34 stars, indicating that the reviews were relatively closely distributed.

Given that a large proportion of the ratings are 4 or 5 stars and thus classified as positive sentiment, it should be noted that a classifier that classified every review as having positive sentiment would have an accuracy of 80.1%. This gives us a good baseline to compare our model’s performance to. 

Method:
	We implemented a neural network algorithm for our sentiment analysis primarily because with lots of data, neural networks can be trained to be very accurate compared to other models, and we were able to scrape around 150,000 reviews from TripAdvisor. The downside of neural networks compared to some other models is that they are slower, but training our model never took longer than 10-20 seconds (except for generating the word embeddings, which took around 30 minutes, but that only had to be done once). We used a 4-layer neural network with the Relu activation function, which is max(0,X), for the 3 hidden layers. The output layer used a sigmoid activation function to generate probabilities for classifying the sentiment of the reviews. The sigmoid activation function is fw, b(x(i))= 11+e-zwhere z=w*x(I)+b. The neural network learned the optimal weights and biases using the binary cross entropy loss function, given by J= i=1m-yilog(ŷi)-(1-yi)log(1-ŷi). Instead of traditional gradient descent, we used the Adaptive Moment Estimation (Adam) optimizer to learn more efficiently.

Preliminary Experiments:
As a baseline model, we used logistic regression with a sigmoid activation function. Our results were as follows. Logistic Regression Train accuracy: 83.4% & Logistic Regression CV accuracy: 83.2% (note that as stated earlier, this outperforms a model that classifies every review as positive by 3.1%). Then, we developed a 4 layer neural network with a Relu activation function for the 3 hidden layers, and a sigmoid activation function for the output layer. After training our neural network model, it successfully produced higher accuracies than the logistic regression model—NN Train accuracy: 86.8% & NN CV accuracy: 86.6%. We then conducted some preliminary experiments to improve these results. Initially, we had a regularization parameter for each layer; however, we realized that since our train set accuracy and cross-validation set accuracy were similar, we had low variance. Thus, we tried removing the regularization parameter to potentially improve our bias and found that as a result, our new accuracies were—NN Train accuracy: 87.1% & NN CV accuracy: 87.6%. We chose the mini-batch size to be 64 for our training set because we wanted to keep our batch size relatively small and a power of 2 (because processors offer a better runtime when the batch size is a power of 2). Our primary metric was accuracy which is the total number of correct classifications of 0 or 1 over the total number of classifications. We also experimented with changing the number of layers in our neural network as well as changing the number of units in each layer of the neural network until we settled on 4 layers: 48 units in the first, 20 units in the second layer, 10 units in the third layer, and 1 unit in the output layer. We computed the precision and recall of the train set—89.8% precision & 95.2% recall—and of the cross-validation set—89.7% precision & 95.0% recall—and computed a confusion matrix for the cross-validation set to see how many true positives, false positives, true negatives, and false negatives we had.

We performed error analysis by manually looking through examples in the cv set and categorizing them by common traits. We looked through all of the misclassified reviews in the first 200 reviews. One thing we noticed was that some negative reviews that used very positive words, like “great” and “prestige,” were misclassified due to the model not understanding sarcasm. Additionally, some misclassifications were not necessarily errors on behalf of our model – some reviews included a mixed bag of positive and negative sentiments, and there were cases where users marked an arbitrary star value that did not align with their written review.
